{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ` TEXT PREPROCESSING`\n",
        "#### Text preprocessing is the initial and most important step in Natural Language Processing (NLP). Raw text data collected from sources like social media, articles, or speech transcripts often contains noise (such as punctuation, stopwords, slang, or irrelevant characters). To make the data clean, structured, and meaningful for machine learning or deep learning models, we apply text preprocessing techniques."
      ],
      "metadata": {
        "id": "ICZMmHZWIEYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  `Import Libraries `\n",
        "#### This code imports essential libraries for text preprocessing in NLP. The nltk library provides tools for tokenization, stemming, lemmatization, and stopword removal. The re module handles regular expressions for cleaning text, while string helps with punctuation processing. From nltk.corpus, stopwords are used to filter out common words, and word_tokenize splits text into tokens. For normalization, PorterStemmer reduces words to their root form, while WordNetLemmatizer converts words to their meaningful base form. Lastly, Counter from collections is used to count word frequencies for analysis."
      ],
      "metadata": {
        "id": "jUUb6qmCJKvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, re, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "qzgmsFQaIlZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Download resources (only once) `\n",
        "#### These commands download necessary NLTK resources.\n",
        "\n",
        "**punkt :** tokenizer for splitting text into sentences and words.\n",
        "\n",
        "**stopwords :** list of common words (e.g., “the”, “is”) to filter out.\n",
        "\n",
        "**averaged_perceptron_tagger :** part-of-speech (POS) tagger to identify grammar roles (noun, verb, etc.).\n",
        "\n",
        "**wordnet :** lexical database used for lemmatization and semantic analysis."
      ],
      "metadata": {
        "id": "ZrUbnbDjJsIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Tk-u97J1Vp",
        "outputId": "51cfbd44-b80b-4b28-9388-22b16d36e424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ` Raw Text `"
      ],
      "metadata": {
        "id": "sDe-8BgNKxOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Data science in Python refers to the application of the Python programming language\n",
        "and its extensive ecosystem of libraries to perform various tasks within the data science workflow.\n",
        "Python has become a dominant language in data science due to its versatility, readability,\n",
        "and the rich set of tools available for data manipulation, analysis, visualization, and machine learning.\n",
        "\"\"\"\n",
        "print(\"=== ORIGINAL RAW TEXT ===\\n\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggrSWGPzK_Es",
        "outputId": "17643cdd-de7d-4b51-d054-26650924dffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ORIGINAL RAW TEXT ===\n",
            " \n",
            "Data science in Python refers to the application of the Python programming language\n",
            "and its extensive ecosystem of libraries to perform various tasks within the data science workflow.\n",
            "Python has become a dominant language in data science due to its versatility, readability,\n",
            "and the rich set of tools available for data manipulation, analysis, visualization, and machine learning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Lowercasing `\n",
        "Converts all text to lowercase so words like “Dog” and “dog” are treated the same, ensuring uniformity."
      ],
      "metadata": {
        "id": "y5Tdl2VtLcm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Text = text.lower()\n",
        "print(\"\\n[1] Lowercased Text:\\n\", Text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv9VM-jMLjuo",
        "outputId": "691fe07c-9451-4af5-96ad-02bdd2d77bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1] Lowercased Text:\n",
            " \n",
            "data science in python refers to the application of the python programming language\n",
            "and its extensive ecosystem of libraries to perform various tasks within the data science workflow.\n",
            "python has become a dominant language in data science due to its versatility, readability,\n",
            "and the rich set of tools available for data manipulation, analysis, visualization, and machine learning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Remove Numbers & Punctuation `\n",
        " Cleans text by deleting digits and punctuation marks, which usually don’t add meaning in NLP tasks."
      ],
      "metadata": {
        "id": "miWsFICrMWPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "print(\"\\n[2] Cleaned Text (no punctuation/numbers):\\n\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHAvsLa2Mffj",
        "outputId": "46a8da47-94cf-44e1-b2c0-02a0d60080ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2] Cleaned Text (no punctuation/numbers):\n",
            " \n",
            "ata science in ython refers to the application of the ython programming language\n",
            "and its extensive ecosystem of libraries to perform various tasks within the data science workflow\n",
            "ython has become a dominant language in data science due to its versatility readability\n",
            "and the rich set of tools available for data manipulation analysis visualization and machine learning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The describe below code processes text step by step to prepare it for NLP tasks. First, the text is tokenized using word_tokenize(), which splits the sentence into individual words or tokens. Next, stopword removal is applied by filtering out common words like “the, is, and” that do not add much meaning. After that, stemming is performed with PorterStemmer(), which reduces words to their root form by cutting off suffixes (e.g., “running” → “run”). To refine this further, lemmatization is applied using WordNetLemmatizer(), which converts words to their proper base form based on grammar and dictionary meanings (e.g., “cars” → “car”, “better” → “good”). Finally, the code applies Part-of-Speech (POS) tagging with nltk.pos_tag(), which labels each word according to its grammatical role, such as noun, verb, or adjective. Together, these steps transform raw text into a clean, structured, and linguistically rich format, making it easier for machine learning models to analyze and understand."
      ],
      "metadata": {
        "id": "Zgg49nFSOkGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Tokenization `\n",
        " Breaks sentences or text into smaller units (tokens), such as words or phrases, making analysis easier."
      ],
      "metadata": {
        "id": "U19G5FzOMphL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\n[3] Tokens:\\n\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh9p0SWPMv-s",
        "outputId": "01709000-eb3f-43a5-d158-af6ec594701e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3] Tokens:\n",
            " ['ata', 'science', 'in', 'ython', 'refers', 'to', 'the', 'application', 'of', 'the', 'ython', 'programming', 'language', 'and', 'its', 'extensive', 'ecosystem', 'of', 'libraries', 'to', 'perform', 'various', 'tasks', 'within', 'the', 'data', 'science', 'workflow', 'ython', 'has', 'become', 'a', 'dominant', 'language', 'in', 'data', 'science', 'due', 'to', 'its', 'versatility', 'readability', 'and', 'the', 'rich', 'set', 'of', 'tools', 'available', 'for', 'data', 'manipulation', 'analysis', 'visualization', 'and', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Stopword Removal`\n",
        " Eliminates common words like “the, is, and” that don’t carry significant meaning in most NLP applications."
      ],
      "metadata": {
        "id": "1-l6raayNCE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokens_nostop = [w for w in tokens if w not in stop_words]\n",
        "print(\"\\n[4] Tokens after Stopword Removal:\\n\", tokens_nostop)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF0drOkgNN1I",
        "outputId": "1add7ab8-cd3b-4552-cfb7-8bf9b5c8d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4] Tokens after Stopword Removal:\n",
            " ['ata', 'science', 'ython', 'refers', 'application', 'ython', 'programming', 'language', 'extensive', 'ecosystem', 'libraries', 'perform', 'various', 'tasks', 'within', 'data', 'science', 'workflow', 'ython', 'become', 'dominant', 'language', 'data', 'science', 'due', 'versatility', 'readability', 'rich', 'set', 'tools', 'available', 'data', 'manipulation', 'analysis', 'visualization', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Stemming `\n",
        "Reduces words to their root by chopping off prefixes/suffixes (e.g., “playing” → “play”), though sometimes less accurate."
      ],
      "metadata": {
        "id": "mfReiCp-NWFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in tokens_nostop]\n",
        "print(\"\\n[5] Stemmed Tokens:\\n\", stemmed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBvsDb0TNdsn",
        "outputId": "cc743dd0-c76f-4b4b-d4f7-93cf8f316727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5] Stemmed Tokens:\n",
            " ['ata', 'scienc', 'ython', 'refer', 'applic', 'ython', 'program', 'languag', 'extens', 'ecosystem', 'librari', 'perform', 'variou', 'task', 'within', 'data', 'scienc', 'workflow', 'ython', 'becom', 'domin', 'languag', 'data', 'scienc', 'due', 'versatil', 'readabl', 'rich', 'set', 'tool', 'avail', 'data', 'manipul', 'analysi', 'visual', 'machin', 'learn']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`Lemmatization `\n",
        "More advanced than stemming; uses vocabulary and grammar rules to convert words to their base form (e.g., “better” → “good”)."
      ],
      "metadata": {
        "id": "s27nxwqSNi97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in tokens_nostop]\n",
        "print(\"\\n[6] Lemmatized Tokens:\\n\", lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viF_3LcUNsTp",
        "outputId": "751b9d92-234f-4192-d5fb-eedfd54e9886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6] Lemmatized Tokens:\n",
            " ['ata', 'science', 'ython', 'refers', 'application', 'ython', 'programming', 'language', 'extensive', 'ecosystem', 'library', 'perform', 'various', 'task', 'within', 'data', 'science', 'workflow', 'ython', 'become', 'dominant', 'language', 'data', 'science', 'due', 'versatility', 'readability', 'rich', 'set', 'tool', 'available', 'data', 'manipulation', 'analysis', 'visualization', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `POS Tagging (Part-of-Speech Tagging) `\n",
        "Labels each word in a sentence with its grammatical role, such as noun, verb, adjective, etc., which helps in deeper text analysis."
      ],
      "metadata": {
        "id": "QnlS_3yXN3a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(lemmatized)\n",
        "print(\"\\n[7] POS Tags:\\n\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qrpB0cjN-k5",
        "outputId": "fc86f240-e0ae-4acf-d647-453b1374fa00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[7] POS Tags:\n",
            " [('ata', 'NNS'), ('science', 'NN'), ('ython', 'NN'), ('refers', 'NNS'), ('application', 'VBP'), ('ython', 'RB'), ('programming', 'VBG'), ('language', 'NN'), ('extensive', 'JJ'), ('ecosystem', 'NN'), ('library', 'JJ'), ('perform', 'NN'), ('various', 'JJ'), ('task', 'NN'), ('within', 'IN'), ('data', 'NNS'), ('science', 'NN'), ('workflow', 'IN'), ('ython', 'NN'), ('become', 'VBN'), ('dominant', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('science', 'NN'), ('due', 'JJ'), ('versatility', 'NN'), ('readability', 'NN'), ('rich', 'JJ'), ('set', 'VBD'), ('tool', 'NN'), ('available', 'JJ'), ('data', 'NNS'), ('manipulation', 'NN'), ('analysis', 'NN'), ('visualization', 'NN'), ('machine', 'NN'), ('learning', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`VOCABULARY & BAG OF WORDS`**\n",
        "#####  This code builds a Vocabulary and Bag of Words (BoW) from a text corpus. First, the corpus sentences are tokenized and converted to lowercase. Then, a vocabulary of unique words is created. Using this vocabulary, the code constructs a BoW matrix, where each row represents a sentence and each column shows the frequency of a word from the vocabulary. In short, it converts text into a numerical format that can be used for NLP and machine learning tasks."
      ],
      "metadata": {
        "id": "1Tp0yjj6OqpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `corpus`\n",
        " is a collection of text or sentences used for NLP tasks. In this code, the corpus is defined as a list containing multiple sentences, which serves as the input data for building the vocabulary and Bag of Words model."
      ],
      "metadata": {
        "id": "8iJAQjt1QBPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I am loving the NLP class, but sometimes it feels confusing!!!\",\n",
        "    \"NLP is a fascinating field — it deals with text, speech, and language understanding.\",\n",
        "    text\n",
        "]"
      ],
      "metadata": {
        "id": "BgHy18h7QI-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code simply prints the corpus in a neat format. Using enumerate(corpus, 1), it numbers each sentence starting from 1, and then displays them one by one with their index."
      ],
      "metadata": {
        "id": "KMkb4ArKQQ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== CORPUS USED ===\")\n",
        "for i, sent in enumerate(corpus, 1):\n",
        "    print(f\"{i}. {sent}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAUaaA-5Qg_B",
        "outputId": "8cbd42ba-6ba7-4124-f5ea-c03c45ba1e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CORPUS USED ===\n",
            "1. I am loving the NLP class, but sometimes it feels confusing!!!\n",
            "2. NLP is a fascinating field — it deals with text, speech, and language understanding.\n",
            "3. \n",
            "ata science in ython refers to the application of the ython programming language\n",
            "and its extensive ecosystem of libraries to perform various tasks within the data science workflow\n",
            "ython has become a dominant language in data science due to its versatility readability\n",
            "and the rich set of tools available for data manipulation analysis visualization and machine learning\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Tokenize each sentence & lowercase`\n",
        "This code takes each sentence in the corpus, converts it to lowercase, and then tokenizes it into individual words using word_tokenize(). The result is a list of tokenized sentences."
      ],
      "metadata": {
        "id": "MgDzsBB9QmFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = [word_tokenize(sent.lower()) for sent in corpus]"
      ],
      "metadata": {
        "id": "xAriiD1fQ5OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Build vocabulary`\n",
        "This code creates the vocabulary from the tokenized corpus. It collects all words from every sentence, removes duplicates using set(), sorts them alphabetically with sorted(), and stores them in vocab. Finally, it prints the list of unique words that make up the vocabulary."
      ],
      "metadata": {
        "id": "8imfWxbQRM9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set([word for sent in tokenized_corpus for word in sent]))\n",
        "print(\"\\n=== Vocabulary ===\")\n",
        "print(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXnMKBg3REFd",
        "outputId": "b92b0d5a-7b64-4683-812f-713f25bc905a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vocabulary ===\n",
            "['!', ',', '.', 'a', 'am', 'analysis', 'and', 'application', 'ata', 'available', 'become', 'but', 'class', 'confusing', 'data', 'deals', 'dominant', 'due', 'ecosystem', 'extensive', 'fascinating', 'feels', 'field', 'for', 'has', 'i', 'in', 'is', 'it', 'its', 'language', 'learning', 'libraries', 'loving', 'machine', 'manipulation', 'nlp', 'of', 'perform', 'programming', 'readability', 'refers', 'rich', 'science', 'set', 'sometimes', 'speech', 'tasks', 'text', 'the', 'to', 'tools', 'understanding', 'various', 'versatility', 'visualization', 'with', 'within', 'workflow', 'ython', '—']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Create Bag of Words matrix`\n",
        "This code builds the Bag of Words (BoW) matrix. For each tokenized sentence, it counts word frequencies using Counter. Then, for every word in the vocabulary, it checks how many times it appears in the sentence (get(word, 0) means 0 if absent). These counts form a row in the BoW matrix, where each row represents a sentence and each column corresponds to a vocabulary word. Finally, it prints the matrix."
      ],
      "metadata": {
        "id": "WDxs0BpORaq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = []\n",
        "for sent in tokenized_corpus:\n",
        "    word_count = Counter(sent)\n",
        "    bow_matrix.append([word_count.get(word, 0) for word in vocab])\n",
        "print(\"\\n=== Bag of Words Matrix ===\")\n",
        "for row in bow_matrix:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX1v7zVrRnaL",
        "outputId": "4d276883-8876-471e-90a1-3ffcef66ceba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Bag of Words Matrix ===\n",
            "[3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 1, 3, 1, 1, 1, 1, 0, 0, 0, 3, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 0, 2, 2, 1, 1, 0, 1, 1, 0, 3, 1, 1, 1, 1, 1, 3, 1, 0, 0, 1, 0, 4, 3, 1, 0, 1, 1, 1, 0, 1, 1, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ` Prepare Data`"
      ],
      "metadata": {
        "id": "uUlyCgbkestE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import math, pandas, and Counter for counting terms.tokenized_docs = preprocessed text documents (list of tokens).N = total number of documents."
      ],
      "metadata": {
        "id": "VQM5X-JUe5rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "tokenized_docs = [\n",
        "    [\"data\",\"science\",\"in\",\"python\",\"refers\",\"to\",\"the\",\"application\",\"of\",\"the\",\"python\",\"programming\",\"language\",\"and\",\"its\",\"extensive\",\"ecosystem\",\"of\",\"libraries\",\"to\",\"perform\",\"various\",\"tasks\",\"within\",\"the\",\"data\",\"science\",\"workflow\"],\n",
        "    [\"python\",\"has\",\"become\",\"a\",\"dominant\",\"language\",\"in\",\"data\",\"science\",\"due\",\"to\",\"its\",\"versatility\",\"readability\",\"and\",\"the\",\"rich\",\"set\",\"of\",\"tools\",\"available\",\"for\",\"data\",\"manipulation\",\"analysis\",\"visualization\",\"and\",\"machine\",\"learning\"]\n",
        "]\n",
        "N = len(tokenized_docs)\n"
      ],
      "metadata": {
        "id": "rAQbMRH_e0B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Build Vocabulary & Term Counters`"
      ],
      "metadata": {
        "id": "cI6RB9e5fjeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Counters,count of each word in each document.vocab list of all unique words across all documents."
      ],
      "metadata": {
        "id": "oeORZY7ngh4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counters = [Counter(doc) for doc in tokenized_docs]\n",
        "vocab = sorted(set(word for doc in tokenized_docs for word in doc))"
      ],
      "metadata": {
        "id": "ZpM8d8lai5Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Compute Inverse Document Frequency (IDF)`"
      ],
      "metadata": {
        "id": "YyuNk6E0jHY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### df = number of documents where each term appears.\n",
        "#### idf = measures how rare or important a term is across all documents.\n",
        "#### Rare words → higher IDF; common words → lower"
      ],
      "metadata": {
        "id": "aoVpaBDTjavK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = {term: sum(1 for c in counters if c[term] > 0) for term in vocab}\n",
        "idf = {term: math.log(N / df[term]) if df[term] > 0 else 0 for term in vocab}"
      ],
      "metadata": {
        "id": "JwdnNGLejomK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Compute Term Frequency (TF) and TF–IDF`"
      ],
      "metadata": {
        "id": "SQ_TuYYSj7bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate TF for each term in each document:\n",
        "####tf_raw = raw count\n",
        "#### tf_norm = count ÷ total words (normalized)\n",
        "\n",
        "\n",
        "\n",
        "#### Multiply TF by IDF → TF–IDF\n",
        "#### Store results in a list of dictionaries for table creation."
      ],
      "metadata": {
        "id": "RE8sEmgMkEaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for term in vocab:\n",
        "    row = {\"term\": term, \"idf\": round(idf[term], 6)}\n",
        "    for i, c in enumerate(counters, 1):\n",
        "        tf_raw = c[term]                          # Raw count in document\n",
        "        total_terms = sum(c.values())             # Total words in document\n",
        "        tf_norm = tf_raw / total_terms if total_terms > 0 else 0   # Normalized TF\n",
        "        tfidf = tf_norm * idf[term]               # TF–IDF = TF × IDF\n",
        "\n",
        "        row[f\"tf_doc{i}\"] = tf_raw\n",
        "        row[f\"tf_norm_doc{i}\"] = round(tf_norm, 6)\n",
        "        row[f\"tfidf_doc{i}\"] = round(tfidf, 6)\n",
        "    rows.append(row)\n"
      ],
      "metadata": {
        "id": "5rBlVabCkUQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Create TF–IDF Table`"
      ],
      "metadata": {
        "id": "euZfpZPBkg0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert TF–IDF results into a pandas DataFrame for easy viewing.\n",
        "\n",
        "#### Columns: Term | IDF | TF (raw) | TF (normalized) | TF–IDF per document.\n",
        "\n",
        "#### Display first 20 rows of the table."
      ],
      "metadata": {
        "id": "aE5EZK2JktwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_table = pd.DataFrame(rows)\n",
        "cols = [\"term\", \"idf\"]\n",
        "for i in range(1, N+1):\n",
        "    cols += [f\"tf_doc{i}\", f\"tf_norm_doc{i}\", f\"tfidf_doc{i}\"]\n",
        "df_table = df_table[cols]\n",
        "print(df_table.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOOb6nLakAtJ",
        "outputId": "ac970a59-16b3-4757-8a2e-c89b42931c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            term       idf  tf_doc1  tf_norm_doc1  tfidf_doc1  tf_doc2  \\\n",
            "0              a  0.693147        0      0.000000    0.000000        1   \n",
            "1       analysis  0.693147        0      0.000000    0.000000        1   \n",
            "2            and  0.000000        1      0.035714    0.000000        2   \n",
            "3    application  0.693147        1      0.035714    0.024755        0   \n",
            "4      available  0.693147        0      0.000000    0.000000        1   \n",
            "5         become  0.693147        0      0.000000    0.000000        1   \n",
            "6           data  0.000000        2      0.071429    0.000000        2   \n",
            "7       dominant  0.693147        0      0.000000    0.000000        1   \n",
            "8            due  0.693147        0      0.000000    0.000000        1   \n",
            "9      ecosystem  0.693147        1      0.035714    0.024755        0   \n",
            "10     extensive  0.693147        1      0.035714    0.024755        0   \n",
            "11           for  0.693147        0      0.000000    0.000000        1   \n",
            "12           has  0.693147        0      0.000000    0.000000        1   \n",
            "13            in  0.000000        1      0.035714    0.000000        1   \n",
            "14           its  0.000000        1      0.035714    0.000000        1   \n",
            "15      language  0.000000        1      0.035714    0.000000        1   \n",
            "16      learning  0.693147        0      0.000000    0.000000        1   \n",
            "17     libraries  0.693147        1      0.035714    0.024755        0   \n",
            "18       machine  0.693147        0      0.000000    0.000000        1   \n",
            "19  manipulation  0.693147        0      0.000000    0.000000        1   \n",
            "\n",
            "    tf_norm_doc2  tfidf_doc2  \n",
            "0       0.034483    0.023902  \n",
            "1       0.034483    0.023902  \n",
            "2       0.068966    0.000000  \n",
            "3       0.000000    0.000000  \n",
            "4       0.034483    0.023902  \n",
            "5       0.034483    0.023902  \n",
            "6       0.068966    0.000000  \n",
            "7       0.034483    0.023902  \n",
            "8       0.034483    0.023902  \n",
            "9       0.000000    0.000000  \n",
            "10      0.000000    0.000000  \n",
            "11      0.034483    0.023902  \n",
            "12      0.034483    0.023902  \n",
            "13      0.034483    0.000000  \n",
            "14      0.034483    0.000000  \n",
            "15      0.034483    0.000000  \n",
            "16      0.034483    0.023902  \n",
            "17      0.000000    0.000000  \n",
            "18      0.034483    0.023902  \n",
            "19      0.034483    0.023902  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnGuvocEgOCE",
        "outputId": "ff81152b-ba56-48de-ffba-0dea2b9996bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "co49UkcPgoTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=texts, vector_size=50, window=3, min_count=1, sg=1)\n",
        "\n",
        "print(\"\\n✅ Word2Vec Vocabulary:\")\n",
        "print(list(w2v_model.wv.index_to_key))\n",
        "\n",
        "print(\"\\n✅ Similar words to 'learning':\")\n",
        "print(w2v_model.wv.most_similar(\"learning\"))"
      ],
      "metadata": {
        "id": "dbw10GaJglyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}